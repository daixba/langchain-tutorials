{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieval Augmented Generation (RAG, 检索增强生成).\n",
    "\n",
    "You can use Retrieval Augmented Generation (RAG) to retrieve data from outside a foundation model and augment your prompts by adding the relevant retrieved data in context. \n",
    "\n",
    "Especially if the data is private and should not be shared in the internet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![RAG](assets/rag.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(image source from SageMaker doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangChain provide many ways to retrieve data from outside of LLMs.  It provides an interface of `Retriever` that returns documents given an unstructured query. \n",
    "\n",
    "Retriever is more general than a vector store. A retriever does not need to be able to store documents, only to return (or retrieve) it. Vector stores can be used as the backbone of a retriever, but there are other types of retrievers as well, such as Amazon Kendra.\n",
    "\n",
    "We will focus on Vector store-backed retriever in this notebook, but you can try others by yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector store-backed retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic Data follow and stages. We will talk about each components that are used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![RAG Flow](assets/qa_flow.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Image from LangChain doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will still use questions about Amazon Bedrock as example. \n",
    "\n",
    "There is a good blog about Bedrock from AWS and we will try to load the document via WebBaseLoader. \n",
    "\n",
    "There are many other loaders (such as PDF, csv, Unstructured data loader) provided by LangChain, you can try that with different sources by yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import WebBaseLoader\n",
    "web_loader = WebBaseLoader(\"https://aws.amazon.com/blogs/machine-learning/announcing-new-tools-for-building-with-generative-ai-on-aws/\")\n",
    "data = web_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For some loaders such as PDF, you can load and split by pages.\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Splitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangChain provides some TextSplitter out of the box, we will use RecursiveCharacterTextSplitter here. You can refer to LangChain doc for other types.\n",
    "\n",
    "The purpose of splitter is to split a long document into smaller chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# split a long document into smaller chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\"],\n",
    "    chunk_size = 512,\n",
    "    chunk_overlap  = 0,\n",
    "    length_function = len,\n",
    "    is_separator_regex = False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check how many chunks\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='. Bedrock will offer the ability to access a range of powerful FMs for text and images—including Amazon’s Titan FMs, which consist of two new LLMs we’re also announcing today—through a scalable, reliable, and secure AWS managed service', metadata={'source': 'https://aws.amazon.com/blogs/machine-learning/announcing-new-tools-for-building-with-generative-ai-on-aws/', 'title': 'Announcing New Tools for Building with Generative AI on AWS | AWS Machine Learning Blog', 'language': 'en-US'})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Randomly pick one, you can open the blog from browser and find out where is this chunk in the blog.\n",
    "docs[30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Embedding Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are using OpenAI, you can simply use `OpenAIEmbeddings` which use the `text-embedding-ada-002` model by default."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embeddings_model = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will use a small sentence transformers embedding model with 384 dimensions. You can try something else on your own, such as BGE etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.020386869087815285,\n",
       " 0.025280870497226715,\n",
       " -0.0005662219482474029,\n",
       " 0.01161543931812048,\n",
       " -0.037988364696502686,\n",
       " -0.11998133361339569,\n",
       " 0.04170951247215271,\n",
       " -0.02085716277360916,\n",
       " -0.05900677293539047,\n",
       " 0.024232564494013786]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  list of texts\n",
    "# embeddings = embeddings_model.embed_documents([\"Hello World!\", \"Who are you?\"])\n",
    "\n",
    "# a single piece of text\n",
    "embeddings = embeddings_model.embed_query(\"Hello World!\")\n",
    "embeddings[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many vector stores supported by LangChain, check https://python.langchain.com/docs/integrations/vectorstores/ for the full list.\n",
    "\n",
    "In this notebook, we will use FAISS.  In the Appendix section, I also provide some example of using OpenSearch as the vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You will need to install FAISS first.\n",
    "!pip install faiss-gpu # For CUDA 7.5+ Supported GPU's.\n",
    "# OR\n",
    "!pip install faiss-cpu # For CPU Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "db = FAISS.from_documents(docs, embeddings_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='. Bedrock also makes it easy to access Stability AI’s suite of text-to-image foundation models, including Stable Diffusion (the most popular of its kind), which is capable of generating unique, realistic, high-quality images, art, logos, and designs.', metadata={'source': 'https://aws.amazon.com/blogs/machine-learning/announcing-new-tools-for-building-with-generative-ai-on-aws/', 'title': 'Announcing New Tools for Building with Generative AI on AWS | AWS Machine Learning Blog', 'language': 'en-US'}),\n",
       " Document(page_content='One of the most important capabilities of Bedrock is how easy it is to customize a model. Customers simply point Bedrock at a few labeled examples in Amazon S3, and the service can fine-tune the model for a particular task without having to annotate large volumes of data (as few as 20 examples is enough). Imagine a content marketing manager who works at a leading fashion retailer and needs to develop fresh, targeted ad and campaign copy for an upcoming new line of handbags', metadata={'source': 'https://aws.amazon.com/blogs/machine-learning/announcing-new-tools-for-building-with-generative-ai-on-aws/', 'title': 'Announcing New Tools for Building with Generative AI on AWS | AWS Machine Learning Blog', 'language': 'en-US'}),\n",
       " Document(page_content='. To do this, they provide Bedrock a few labeled examples of their best performing taglines from past campaigns, along with the associated product descriptions. Bedrock makes a separate copy of the base foundational model that is accessible only to the customer and trains this private copy of the model. After training, Bedrock will automatically start generating effective social media, display ad, and web copy for the new handbags', metadata={'source': 'https://aws.amazon.com/blogs/machine-learning/announcing-new-tools-for-building-with-generative-ai-on-aws/', 'title': 'Announcing New Tools for Building with Generative AI on AWS | AWS Machine Learning Blog', 'language': 'en-US'}),\n",
       " Document(page_content='Bedrock is now in limited preview, and customers like Coda are excited about how fast their development teams have gotten up and running. Shishir Mehrotra, Co-founder and CEO of Coda, says, “As a longtime happy AWS customer, we’re excited about how Amazon Bedrock can bring quality, scalability, and performance to Coda AI. Since all our data is already on AWS, we are able to quickly incorporate generative AI using Bedrock, with all the security and privacy we need to protect our data built-in', metadata={'source': 'https://aws.amazon.com/blogs/machine-learning/announcing-new-tools-for-building-with-generative-ai-on-aws/', 'title': 'Announcing New Tools for Building with Generative AI on AWS | AWS Machine Learning Blog', 'language': 'en-US'})]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "query = \"What is Bedrock?\"\n",
    "query_result = db.similarity_search(query)\n",
    "# query_result = db.similarity_search_with_score(query)\n",
    "query_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='. Bedrock also makes it easy to access Stability AI’s suite of text-to-image foundation models, including Stable Diffusion (the most popular of its kind), which is capable of generating unique, realistic, high-quality images, art, logos, and designs.', metadata={'source': 'https://aws.amazon.com/blogs/machine-learning/announcing-new-tools-for-building-with-generative-ai-on-aws/', 'title': 'Announcing New Tools for Building with Generative AI on AWS | AWS Machine Learning Blog', 'language': 'en-US'}),\n",
       " Document(page_content='One of the most important capabilities of Bedrock is how easy it is to customize a model. Customers simply point Bedrock at a few labeled examples in Amazon S3, and the service can fine-tune the model for a particular task without having to annotate large volumes of data (as few as 20 examples is enough). Imagine a content marketing manager who works at a leading fashion retailer and needs to develop fresh, targeted ad and campaign copy for an upcoming new line of handbags', metadata={'source': 'https://aws.amazon.com/blogs/machine-learning/announcing-new-tools-for-building-with-generative-ai-on-aws/', 'title': 'Announcing New Tools for Building with Generative AI on AWS | AWS Machine Learning Blog', 'language': 'en-US'}),\n",
       " Document(page_content='. Independent Software Vendors (ISVs) like C3 AI and Pega are excited to leverage Bedrock for easy access to its great selection of FMs with all of the security, privacy, and reliability they expect from AWS.', metadata={'source': 'https://aws.amazon.com/blogs/machine-learning/announcing-new-tools-for-building-with-generative-ai-on-aws/', 'title': 'Announcing New Tools for Building with Generative AI on AWS | AWS Machine Learning Blog', 'language': 'en-US'}),\n",
       " Document(page_content='Bedrock customers can choose from some of the most cutting-edge FMs available today. This includes the Jurassic-2 family of multilingual LLMs from AI21 Labs, which follow natural language instructions to generate text in Spanish, French, German, Portuguese, Italian, and Dutch. Claude, Anthropic’s LLM, can perform a wide variety of conversational and text processing tasks and is based on Anthropic’s extensive research into training honest and responsible AI systems', metadata={'source': 'https://aws.amazon.com/blogs/machine-learning/announcing-new-tools-for-building-with-generative-ai-on-aws/', 'title': 'Announcing New Tools for Building with Generative AI on AWS | AWS Machine Learning Blog', 'language': 'en-US'})]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Maximum marginal relevance search (MMR) - 最大边际相关性检索\n",
    "mmr_query_result = db.max_marginal_relevance_search(query, k=4, fetch_k=10)\n",
    "mmr_query_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can persist the data for later use.\n",
    "db.save_local(\"faiss_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After that, you can load from local. For that, you don't have to load the source and split again.\n",
    "db = FAISS.load_local(\"faiss_index\", embeddings_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can simply use the vector store as retriever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='. Bedrock will offer the ability to access a range of powerful FMs for text and images—including Amazon’s Titan FMs, which consist of two new LLMs we’re also announcing today—through a scalable, reliable, and secure AWS managed service', metadata={'source': 'https://aws.amazon.com/blogs/machine-learning/announcing-new-tools-for-building-with-generative-ai-on-aws/', 'title': 'Announcing New Tools for Building with Generative AI on AWS | AWS Machine Learning Blog', 'language': 'en-US'}),\n",
       " Document(page_content='We took all of that feedback from customers, and today we are excited to announce Amazon Bedrock, a new service that makes FMs from AI21 Labs, Anthropic, Stability AI, and Amazon accessible via an API. Bedrock is the easiest way for customers to build and scale generative AI-based applications using FMs, democratizing access for all builders', metadata={'source': 'https://aws.amazon.com/blogs/machine-learning/announcing-new-tools-for-building-with-generative-ai-on-aws/', 'title': 'Announcing New Tools for Building with Generative AI on AWS | AWS Machine Learning Blog', 'language': 'en-US'}),\n",
       " Document(page_content='. With Bedrock’s serverless experience, customers can easily find the right model for what they’re trying to get done, get started quickly, privately customize FMs with their own data, and easily integrate and deploy them into their applications using the AWS tools and capabilities they are familiar with, without having to manage any infrastructure (including integrations with Amazon SageMaker ML features like Experiments to test different models and Pipelines to manage their FMs at scale).', metadata={'source': 'https://aws.amazon.com/blogs/machine-learning/announcing-new-tools-for-building-with-generative-ai-on-aws/', 'title': 'Announcing New Tools for Building with Generative AI on AWS | AWS Machine Learning Blog', 'language': 'en-US'}),\n",
       " Document(page_content='. Independent Software Vendors (ISVs) like C3 AI and Pega are excited to leverage Bedrock for easy access to its great selection of FMs with all of the security, privacy, and reliability they expect from AWS.', metadata={'source': 'https://aws.amazon.com/blogs/machine-learning/announcing-new-tools-for-building-with-generative-ai-on-aws/', 'title': 'Announcing New Tools for Building with Generative AI on AWS | AWS Machine Learning Blog', 'language': 'en-US'})]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 4})\n",
    "docs = retriever.get_relevant_documents(\"What FMs are supported by bedrock?\")\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are more examples of defining the retrievers"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Maximum Marginal Relevance Retrieval\n",
    "retriever = db.as_retriever(search_type=\"mmr\", search_kwargs={'k': 2, 'fetch_k': 10})\n",
    "\n",
    "# Similarity Score Threshold Retrieval\n",
    "retriever = db.as_retriever(search_type=\"similarity_score_threshold\", search_kwargs={\"score_threshold\": .5})\n",
    "\n",
    "# top k Retrieval\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RetrievalQA Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have define the vector based retriever, and we can use it with LLMs to generate outputs. We will still use Llama-2 as the LLM here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.55s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "pipe = pipeline(\n",
    "    task=\"text-generation\",\n",
    "    model=\"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    temperature=0.0, \n",
    "    max_new_tokens=1024,\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(llm, retriever=retriever, chain_type=\"stuff\", verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Based on the provided context, Bedrock supports FMs from AI21 Labs, Anthropic, Stability AI, and Amazon.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What FMs are supported by bedrock?\"\n",
    "\n",
    "qa_chain.run(query=query)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out different types of chain type： https://python.langchain.com/docs/modules/chains/document/stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also add memery to the QA chain, for example"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "qa_chain = RetrievalQA.from_chain_type(llm, retriever=retriever, memory=memory, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix - OpenSearch as Vector Store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section is optional, only if you are interested in OpenSearch as vector store.\n",
    "\n",
    "OpenSearch supports three different methods for obtaining the k-nearest neighbors from an index of vectors: \n",
    "- Approximate k-NN\n",
    "- Script Score k-NN (exact k-NN search)\n",
    "- Painless extensions \n",
    "\n",
    "Check https://opensearch.org/docs/latest/search-plugins/knn/index/ for more details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Approximate k-NN search methods leveraged by OpenSearch use approximate nearest neighbor (ANN) algorithms from the nmslib, faiss, and Lucene libraries to power k-NN search. It is the best choice for searches over large indexes (that is, hundreds of thousands of vectors or more) that require low latency. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!pip install opensearch-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import OpenSearchVectorSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Amazon OpenSearch Service (AOS) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opensearchpy import OpenSearch, RequestsHttpConnection, AWSV4SignerAuth\n",
    "import boto3\n",
    "\n",
    "# Use aoss for Serverless and es for OpenSearch.\n",
    "service = \"aoss\"\n",
    "region = \"us-west-2\"\n",
    "\n",
    "opensearch_url = \"https://xxx.us-west-2.aoss.amazonaws.com\"  # replace this url with your own\n",
    "credentials = boto3.Session().get_credentials()\n",
    "awsauth = AWSV4SignerAuth(credentials, region, service)\n",
    "\n",
    "docsearch = OpenSearchVectorSearch.from_documents(\n",
    "    docs,\n",
    "    embeddings_model,\n",
    "    opensearch_url=opensearch_url,\n",
    "    http_auth=awsauth,\n",
    "    timeout = 300,\n",
    "    use_ssl = True,\n",
    "    verify_certs = True,\n",
    "    connection_class = RequestsHttpConnection,\n",
    "    index_name=\"test-vector-index\",\n",
    "    engine=\"faiss\",\n",
    "    bulk_size=512,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_result = docsearch.similarity_search(\n",
    "    query,\n",
    "    k=4,\n",
    ")\n",
    "query_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For OpenSearch Local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docsearch = OpenSearchVectorSearch.from_documents(\n",
    "    docs,\n",
    "    embeddings_model,\n",
    "    opensearch_url=\"https://localhost:9200\",\n",
    "    http_auth=(\"admin\", \"admin\"),\n",
    "    use_ssl = False,\n",
    "    verify_certs = False,\n",
    "    ssl_assert_hostname = False,\n",
    "    ssl_show_warn = False,\n",
    "    bulk_size=512,\n",
    "    index_name=\"test-vector-index1\",\n",
    "\n",
    "    engine=\"faiss\",\n",
    "    space_type=\"innerproduct\",\n",
    "    ef_construction=256,\n",
    "    m=48,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarity_search using Approximate k-NN Search with Custom Parameters\n",
    "query_result = docsearch.similarity_search(\n",
    "    query,\n",
    "    k=4,\n",
    ")\n",
    "query_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run `GET test-vector-index1` from Dev Tools in OpenSearch Dashboard"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "{\n",
    "  \"test-vector-index1\": {\n",
    "    \"aliases\": {},\n",
    "    \"mappings\": {\n",
    "      \"properties\": {\n",
    "        ...\n",
    "        \"text\": {\n",
    "          \"type\": \"text\",\n",
    "          \"fields\": {\n",
    "            \"keyword\": {\n",
    "              \"type\": \"keyword\",\n",
    "              \"ignore_above\": 256\n",
    "            }\n",
    "          }\n",
    "        },\n",
    "        \"vector_field\": {\n",
    "          \"type\": \"knn_vector\",\n",
    "          \"dimension\": 384,\n",
    "          \"method\": {\n",
    "            \"engine\": \"faiss\",\n",
    "            \"space_type\": \"innerproduct\",\n",
    "            \"name\": \"hnsw\",\n",
    "            \"parameters\": {\n",
    "              \"ef_construction\": 256,\n",
    "              \"m\": 48\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    },\n",
    "    ...\n",
    "  }\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
